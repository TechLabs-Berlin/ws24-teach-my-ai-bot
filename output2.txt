ale interaction between patients and LLMs specialized for diagnostic dialogue,
was unfamiliar to PCPs for remote consultation. Thus our study should not be regarded as representative of
usual practice in (tele)medicine.
|3
AI
Figure 2 | Overview of randomized study design. A primary care physician (PCP) and AMIE perform (in a randomized
order) a virtual remote Objective Structured Clinical Examination (OSCE) with simulated patients via online multi-turn
synchronous text chat and produce answers to a post-questionnaire. Both the PCP and AMIE are then evaluated by both the
patient actors as well as specialist physicians.
2 AMIE: An LLM based AI System for Diagnostic Dialogue
In the following sections, we describe the real-world datasets, simulated self-play environment, fine-tuning
process, and inference time chain-of-reasoning that we designed to optimize AMIE for diagnostic conversation
capabilities and clinical communication skills.
2.1 Real-world Datasets for AMIE
AMIE was developed using a diverse suite of real-world datasets including multiple-choice medical question-
answering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries, and
large-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue
generation tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning, and
summarization tasks.
Medical Reasoning. We used the MedQA (multiple-choice) dataset consisting of US Medical Licensing
Examination (USMLE) multiple-choice style open domain questions with four or five possible answers [21].
The training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191
MedQA questions from the training set where clinical experts crafted step-by-step reasoning leading to the
correct answer [13].
Long-form Medical Question Answering. The dataset used here consisted of expert-crafted long-form
responses to 64 questions from HealthSearchQA, LiveQA, and Medication QA in MultiMedBench [12].
|4
Medical Summarization. A dataset consisting of 65 clinician-written summaries of medical notes from
MIMIC-III, a large, publicly available database containing medical records of intensive care unit patients [22],
was used as additional training data for AMIE. MIMIC-III contains approximately 2 million notes spanning
13 types including cardiology, respiratory, radiology, physician, general, discharge, case management, consult,
nursing, pharmacy, nutrition, rehabilitation and social work. 5 notes from each category were selected, with a
minimum total length of 400 tokens and at least one nursing note per patient. Clinicians were instructed to
write abstractive summaries of individual medical notes, capturing key information while also permitting the
inclusion of new informative and clarifying phrases and sentences not present in the original note.
Real-world Dialogue. Here, we used a de-identified dataset licensed from a dialogue research organisation
comprising 98,919 audio transcripts of medical conversations during in-person clinical visits from over 1,000
clinicians over a 10-year period in the United States [23]. It covered 51 medical specialties (primary care,
rheumatology, hematology, oncology, internal medicine and psychiatry among others) and 168 medical
conditions and visit reasons (type II diabetes, rheumatoid arthritis, asthma, depression among the common
conditions). Audio transcripts contained utterances from different speaker roles such as doctors, patients, and
nurses. On average a conversation had 149.8 turns (P0.25 = 75.0, P0.75 = 196.0). For each conversation, the
metadata contained information about patient demographics, reason for the visit (follow-up for pre-existing
condition, acute needs, annual exam and more), and diagnosis type (new, existing or other unrelated). We
refer to [23] for more details.
For this study, we selected dialogues involving only doctors and patients, but not other roles such as nurses.
During preprocessing, we removed paraverbal annotations such as “[LAUGHING]” and “[INAUDIBLE]” from
the transcripts. We then divided the dataset into training (90%) and validation (10%) sets using stratified
sampling based on condition categories and reasons for visits, resulting in 89,027 conversations for training
and 9,892 for validation.
2.2 Simulated Dialogue Learning Environment and Self-play for AMIE
While passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two
substantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world
data often fails to capture the vast range of medical conditions and scenarios, hindering its scalability and
comprehensiveness; (2) the data derived from real-world dialogue transcripts tends to be noisy, containing
ambiguous language (including slang, jargon, and sarcasm), interruptions, ungrammatical utterances, and
implicit references. This in turn, may limit AMIE’s knowledge, capabilities, and applicability.
To address these limitations, we designed a self-play based simulated learning environment for diagnostic
medical dialogues in a virtual care setting, enabling us to scale AMIE’s knowledge and capabilities across a
multitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an
evolving set of simulated dialogues in addition to the static corpus of medical QA, reasoning, summarization,
and real-world dialogue data described above (see Figure 1).
This process consisted of two self-play loops:
• An “inner” self-play loop where AMIE leveraged in-context critic feedback to refine its behavior on
simulated conversations with an AI patient agent.
• An “outer” self-play loop where the set of refined simulated dialogues were incorporated into
subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the
inner loop again, creating a continuous learning cycle.
|5
Simulated Dialogues. At each iteration of fine-tuning, we produced 11,686 dialogues, stemming from 5,230
different medical conditions. Conditions were selected from three datasets:
• Health QA dataset [12] which contained 613 common medical conditions.
• MalaCards Human Disease Database1 which contained 18,455 less common disease conditions.
• MedicineNet Diseases & Conditions Index2 which contained 4,617 less common conditions.
At each self-play iteration, four conversations were generated from each of the 613 common conditions,
while two conversations were generated from each of the 4,617 less common conditions randomly chosen
from MedicineNet and MalaCards. The average simulated dialogue conversation length was 21.28 turns
(P0.25 = 19.0, P0.75 = 25.0).
Using simulated dialogues allowed us to address the limited availability of high-quality, labelled real-world
conversation data and improved the model’s generalization and adaptability to diverse medical contexts. By
leveraging this self-play paradigm, AMIE could continuously learn and refine its conversational and diagnostic
capabilities during patient interactions.
2.2.1 Simulated Dialogue Data Curation
In order to produce high-quality simulated dialogues at scale, we developed a novel multi-agent framework
which comprised three key components:
• Vignette Generator: AMIE leverages web searches to craft unique patient vignettes given a specific
medical condition.
• Simulated Dialogue Generator: Three LLM agents play the roles of patient agent, doctor agent,
and moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions.
• Self-play Critic: A fourth LLM agent acts as a critic to give feedback to the doctor agent for self-
improvement. Notably, AMIE acted as all agents in this framework. We describe each component in
detail below.
Vignette Generator. The vignette generator aimed to create varied and realistic patient scenarios at
scale, which could be subsequently used as context for generating simulated doctor-patient dialogues thereby
allowing AMIE to undergo a training process emulating exposure to a greater number of conditions and
patient backgrounds. The patient vignette (scenario) included essential background information such as
patient demographics, symptoms, past medical history, past surgical history, past social history, and patient
questions, as well as an associated diagnosis and management plan.
For a given condition, patient vignettes were constructed using the following process. First, we retrieved 60
passages (20 each) on the range of demographics, symptoms, and management plans associated with the
condition from using an internet search engine. To ensure these passages were relevant to the given condition,
we used the general-purpose LLM, PaLM-2 [10], to filter these retrieved passages, removing any passages
deemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes
aligned with the demographics, symptoms, and management plans retrieved from the filtered passages, by
providing a one-shot exemplar to enforce a particular vignette format. The prompts for each of these steps
are as follows:
1https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json
2https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json
|6
Search Retrieval Template
What are the specific patient demographics/symptoms/management plan for the condition [Condition]?
Passage Filtering Template
For the clinical condition, [Condition], is the following a good description of common demograph-
ics/symptoms/management plans (Yes/No)?
Description: [Retrieved Passage]
Answer (Yes/No):
Vignette Generation Template
The following are several passages about the demographics, symptoms, and management plan for a
given condition. Generate 2 different patient vignettes consistent with these passages. Follow the
format of