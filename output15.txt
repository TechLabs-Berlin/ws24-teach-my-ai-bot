P
Cardiovascular
Gastroenterology
Internal Medicine
Neurology
Respiratory
OBGYN / Urology
0.2
0.4
0.6
0.8
1.0
Top 3 Matches Ground Truth
AMIE
PCP
Cardiovascular
Gastroenterology
Internal Medicine
Neurology
Respiratory
OBGYN / Urology
0.2
0.4
0.6
0.8
1.0
Top 5 Matches Ground Truth
AMIE
PCP
Cardiovascular
Gastroenterology
Internal Medicine
Neurology
Respiratory
OBGYN / Urology
0.2
0.4
0.6
0.8
1.0
Top 10 Matches Ground Truth
AMIE
PCP
Figure A.8 | Specialist rated DDx accuracy by scenario specialty. Top 1/3/5/10 accuracy for scenarios of each specialty.
Accuracies are based on the specialist ratings for AMIE and PCP differential diagnoses with respect to the ground truth. Number
of dialogues per OSCE agent: Cardiology (29), Gastroenterology (31), Internal Medicine (14), Neurology (30), Respiratory (30),
OBGYN / Urology (15).
|36
A.7 Auto-evaluation on DDx
Here we report the top-k DDx accuracy as computed by the auto-evaluation method. For each DDx in
the DDx list generated by AMIE and PCPs, we used Med-PaLM 2 to determine whether the ground
truth diagnosis appears within the top-k positions of the differential diagnosis list. Given a prediction and
label, the auto-evaluator computes whether they match by prompting Med-PaLM 2 with the following question:
DDx Auto-evaluation Prompt
Is our predicted diagnosis correct (Y/N)? It is okay if the predicted diagnosis is more specific/detailed.
Predicted diagnosis: prediction, True diagnosis: label
Answer [Y/N]:
A.7.1 Reproducing DDx Accuracy via Auto-evaluation
The overall performance trends obtained through the auto-evaluator align well with specialist assessments
in Figure 3 despite marginal differences in the computed accuracy values, as shown in Figure A.9. These
results demonstrate that the auto-evaluator is a valid surrogate for the specialist raters.
a
b
Figure A.9 | Auto-evaluation rated DDx accuracy on all cases. (a) Top-k auto-evaluation rating of AMIE and PCP
with respect to the ground truth. Significant (with FDR correction) for k > 2. (b) Top-k auto-evaluation rating of AMIE and
PCP with respect to the accepted differential. Significant (with FDR correction) for k > 4.
|37
A.7.2 AMIE DDx Accuracy on AMIE and PCP Consultations
We compared AMIE’s diagnosis accuracy based on its own consultations with its accuracy generated from
corresponding PCP consultations, using the DDx auto-evaluator. Results in Figure A.10 showed that the
diagnostic quality remained consistent regardless of whether AMIE processed information from its own
dialogue or from the PCP’s conversation.
a
b
Figure A.10 | Auto-evaluation rated DDx accuracy for AMIE-produced differential diagnoses from the PCP’s
and AMIE’s consultations. AMIE was asked to create a DDx from both the PCP’s and AMIE’s consultations. (a) Top-k
auto-evaluation rating of AMIE DDx on AMIE and PCP consultations with respect to the ground truth. No differences are
statistically significant. (b) Top-k auto-evaluation rating of AMIE DDx on AMIE and PCP consultations with respect to the
accepted differential. No differences are statistically significant.
|38
A.7.3 DDx Accuracy as a Function of Dialogue Turns
Distribution of words and turns. Figure A.11 shows the distributions of words and turns for the OSCE
conversations. Because the number of patient actor words and turns is consistent between groups, neither
agent has an unfair advantage in terms of the amount of information used to make a diagnosis. However, it is
important to note that AMIE is far more verbose in its responses which may have influenced the qualitative
ratings from specialists.
a
b
c
Figure A.11 | Distribution of words and turns in OSCE consultations. (a) Total patient actor words elicited by
AMIE vs. PCPs. (b) Total words sent to patient actor from AMIE vs. PCPs. (c) Total number of turns in AMIE vs. PCP
consultations.
Accuracy by number of turns. Here we plotted the auto-evaluation of AMIE-generated differential
diagnoses as a function of number of turns. We truncated conversations to the first T turns, and then asked
AMIE to produce a DDx with this truncated conversation. For both the AMIE and PCP conversations,
we observed that AMIE’s average diagnostic accuracy began to plateau within 10 turns, with additional
information gathering having diminishing returns on the diagnostic performance as shown in Figure A.12.
a
b
Figure A.12 | Auto-evaluation rated DDx (top-3) accuracy as a function of consultation turns provided to the
model. (a) Top-3 auto-evaluation DDx accuracy as a function of the number of turns for the AMIE DDx on AMIE and PCP
consultations with respect to the ground truth. (b) Top-3 auto-evaluation DDx accuracy as a function of the number of turns for
the AMIE DDx on AMIE and PCP consultations with respect to the the accepted differential. No differences are statistically
significant.
|39
A.8 DDx Accuracy by Location
Accuracy by Location. We compared the specialist ratings for the 67 scenarios conducted in Canada and
the 82 scenarios conducted in India.
a
b
Figure A.13 | Specialist rated DDx accuracy by location. (a) Specialist DDx rating of AMIE and PCP with respect to
the ground truth for the 67 cases conducted in Canada. Accuracies at all k positions are significant with FDR correction. (b)
Specialist DDx rating of AMIE and PCP with respect to the ground truth for the 82 cases conducted in India. While the trends
are the same as in Canada, the differences between AMIE and PCP are not statistically significant with FDR correction.
Shared Scenarios. We repeated 40 of the scenarios at the other location, meaning if it was originally run in
Canada, we then ran it in India and vice-versa. This included all of the UK scenarios and 26 of the India
scenarios. Because these conversations did not have specialist ratings, we instead leveraged auto-evaluation to
compare the produced differential diagnoses and ablate the effect of the OSCE location.
a
b
Figure A.14 | Auto-evaluation rated DDx accuracy for scenarios conducted at both testing locations.
(a)
Auto-evaluation rated top-k DDx performance of AMIE on a set of 40 scenarios conducted in both locations. (b) Auto-evaluation
rated top-k DDx performance of the PCPs on a set of 40 scenarios conducted in both locations.
Results. We observed a higher average diagnostic performance for AMIE in Canada than in India (see Fig-
ure A.13). However, when comparing scenarios performed at both study locations, we observed that AMIE
and PCP performance remained consistent regardless of the study location (see Figure A.14), suggesting that
the observed performance variations are likely not due to the patient actor or clinician rater differences, but
instead might be attributed to inherent differences in the difficulty levels of scenarios in each location.
|40
A.9 Model-based Auto-evaluation of Qualitative Criteria
In order to accurately emulate the ratings of specialists on the clinical criteria in our OSCE evaluation
framework, we developed a model-based auto-evaluation procedure leveraging the AMIE model to score
dialogues from 1 to 5 based on how well they exemplified those qualitative criteria. We initially focused on a
subset of four clinical axes from the PACES criteria (see Table A.2), however, this procedure can be easily
extended to other ratings.
Using the 298 dialogues produced by AMIE and PCPs in this study, we corroborated the results of the
specialist ratings on these 4 criteria using the auto-evaluation procedure. We validated that the auto-evaluation
rankings were well aligned with these specialist ratings (see Figures A.17 and A.18). Additionally, we applied
it to the simulated dialogues generated via the inner-loop self-play procedure to test whether this iterative
process resulted in measurable improvements in dialogue quality (see Figure A.19).
Self-CoT Procedure for Auto-Evaluation of Clinical Criteria. The auto-evaluation procedure we
employed was a two-step process in which we prompted AMIE itself to rate dialogues on the chosen subset of
the PACES criteria (see Table A.2).
1. First, we prompted AMIE to summarize good and bad aspects of several dialogues and provide an
explanation of the provided human rating between 1 and 5 (see Figure A.15).
2. Next, we used these self-generated explanations alongside their respective dialogues as examples in a
5-shot prompt to evaluate and rate a new dialogue. This few-shot prompt included one example for
each point on the 5-point rating scale (see Figure A.16).
In both prompts, we included the rating scale and expert-derived examples of good or bad behaviour for a
particular criterion, matching those shown in Table A.2. We referred to this prompting method as self-CoT
(Chain-of-Thoughts) [1] as the plausible reasoning for the human ratings are derived from the model itself.
Rank-order Agreement. We evaluated our auto-evaluation method by quantifying its agreement with the
specialist rankings of the OSCE dialogues. We limited our analysis to the 149 dialogue pairs in the study.
Thus, each pair consisted of a AMIE conversation and a PCP conversation with the same patient actor, and
rated by the same specialist. For a pair of two dialogues, the three possibilities were: the first one was rated
better than the second one, they were equally rated, or the first one was rated worse than the second one. We
defined the rank-order agreement as the proportion of dialogue pairs for which the specialist ranking was
preserved by the auto-evaluation ratings. For example, we counted it as correct when our auto-evaluation
rated AMIE’s dialogue as better than the PCP’s dialogue if specialists also rated AMIE’s dialogue as better,
regardless of the exact scores each method assigned.
Auto-evaluation Prompting Strategies. Using the rank-order agreement metric, we ablated the effect of
the two-step prompting and compared it to other methods such as the five-shot prompting (i.e. dropping step
1), shuffled five-shot self-CoT prompting where the order of support examples was ra