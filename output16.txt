ndomised each time, and
0-shot prompting using only the rating scale explanation itself (see Figure A.17). All methods outperformed
the chance level, with the two-step process generally outperforming other methods, though this difference was
marginal. Shuffling the examples in the self-CoT prompt made no difference on average.
Benchmarking Auto-evaluation. While auto-evaluation was significantly better than random guessing at
aligning with specialist preferences, it was unclear if the resulting performance was sufficient. To test this, we
had 139 dialogue pairs each rated by a second specialist, and we computed the rank-order agreement of the
two specialists on this subset (see figure A.18). We observed that auto-evaluation was about as accurate as
the alternative specialists in predicting the first specialist’s rank-order, suggesting that it is useful to leverage
auto-evaluation for these criteria.
Evaluating Self-play Dialogues. We applied our auto-evaluation procedure to 1,142 dialogues (derived
from common conditions) before and after being refined through the self-play critique. We demonstrated that,
on average, the refined dialogues after an iteration of critique/revision were rated higher than the original
baseline dialogues across all criteria A.19.
|41
Figure A.15 | Prompt for the explanation generation given an dialogue and the human rating. AMIE was prompted
to summarize good and bad aspects of a particular dialogue and provide an explanation of the given human rating between 1 and
5.
Prompt for Explanation Generation for Dialogue and Human Rating
I have a doctor-patient dialogue and the corresponding rating that quantifies its quality according to
the following criterion: <criterion> (e.g., maintaining patient welfare). The rating of the dialogue is
on a scale of 1 to 5 where:
5: <definition> e.g., “Treats patient respectfully, and ensures comfort, safety and dignity”
1: <definition> e.g., “Causes patient physical or emotional discomfort AND jeopardises patient safety”
First, describe which parts of the dialogue are good with respect to the criterion.
Then, de-
scribe which parts are bad with respect to the criterion. Lastly, summarise the above to explain the
provided rating, using the following format:
Good: ...
Bad: ...
Summary: ...
DIALOGUE: <dialogue>
Rating: <human rating>
EVALUATION:
Example output (for a dialogue with rating 4 on ‘maintaining patient welfare’):
Good: The doctor took the patient’s concerns seriously and acted quickly to address the situation.
They asked pertinent questions to gather information about the patient’s symptoms and medical
history. They also provided clear instructions on what the patient needed to do next.
Bad:
The doctor did not provide much emotional support to the patient.
They could have
been more reassuring and empathetic towards the patient’s fear and anxiety.
Summary:
Overall, the doctor did a good job of maintaining patient welfare in this situa-
tion.
They took prompt action to address the patient’s medical emergency and provided clear
instructions to the patient. However, they could have been more attentive to the patient’s emotional
needs.
|42
Figure A.16 | Prompt for the auto-evaluation of qualitative criteria. We constructed few-shot prompt with self-generated
explanations alongside their respective dialogues as exemplars to evaluate and rate a new dialogue. This few-shot prompt included
one example for each point on the 5-point rating scale.
Prompt for Auto-Evaluation
I have a doctor-patient dialogue which I would like you to evaluate on the following criterion:
<criterion> (e.g., maintaining patient welfare). The dialogue should be rated on a scale of 1-5 with
respect to the criterion where:
5: <definition> e.g., “Treats patient respectfully, and ensures comfort, safety and dignity”
1: <definition> e.g., “Causes patient physical or emotional discomfort AND jeopardises patient safety”
Here are some example dialogues and their ratings:
DIALOGUE: <example dialog>
EVALUATION: <example self-generated explanation>
Rating: <example rating>
...
Now, please rate the following dialogue as instructed below. First, describe which parts of the dialogue
are good with respect to the criterion. Then, describe which parts are bad with respect to the criterion.
Third, summarise the above findings. Lastly, rate the dialogue on a scale of 1-5 with respect to the
criterion, according to this schema:
Good: ...
Bad: ...
Summary: ...
Rating: ...
DIALOGUE: <dialogue>
EVALUATION:
|43
A.9.1 Rank-order Agreement of Auto-evaluation to Specialist
0.0
0.2
0.4
0.6
0.8
1.0
Rank-order Agreement with Specialist
Showing Empathy
Seeking and Addressing Concerns
Maintaining Patient Welfare
Confirming Knowledge and Understanding
Random
0 shot
5 shot
Self-CoT
Self-CoT
(Shuffled)
Figure A.17 | Rank-order agreement to specialist ratings of all 149 dialogue pairs, comparing various auto-
evaluation prompting techniques. We choose to leverage the self-CoT technique for the auto-evaluation of clinical criteria.
0.0
0.2
0.4
0.6
0.8
1.0
Rank-order Agreement with Specialist
Showing Empathy
Seeking and Addressing Concerns
Maintaining Patient Welfare
Confirming Knowledge and Understanding
Random
Random w/ Prevalence
Auto-evaluation
Alternative Specialist
Figure A.18 | Rank-order agreement to specialist ratings of 139 dialogue pairs (excluding cases without multiple
specialist ratings) for alternative specialists compared to the self-CoT auto-evaluation technique. Auto-evaluation
agreement to the first specialist is comparable to inter-specialist agreement. The black dashed line shows the rank-order agreement
one would get with a random ranking of the AMIE and PCP dialogues, while the green dashed line shows the rank-order
agreement with a strategy of randomly guessing according to the distribution of specialist preferences for each criteria.
|44
A.9.2 Auto-evaluation of Simulated Dialogues with Self-play
0.0
0.2
0.4
0.6
0.8
1.0
Proportion Preferred by Auto-evaluation
Confirming Knowledge and Understanding
Maintaining Patient Welfare
Seeking and Addressing Concerns
Showing Empathy
0.22
0.18
0.21
0.21
0.48
0.58
0.53
0.46
0.3
0.24
0.27
0.33
Baseline
Tie
Self-play
Figure A.19 | Comparison of the simulated dialogue quality before and after self-play as assessed by auto-
evaluation. The self-play dialogues after one round of critique are preferred by auto-evaluation more often than the baseline
dialogue generated without revision.
|45
Table A.5 | Model card for AMIE.
Model characteristics
Model initialization
The model was initialized from an LLM similar to PaLM-2 [2]. Addi-
tional fine-tuning was performed as described in Section 2.3.
Usage
Application
The primary use is research on LLMs for clinical history-taking and
medical dialogue including advancing diagnostic accuracy, alignment
methods, fairness, safety, and equity research, and understanding limi-
tations of current LLMs for such applications.
Data overview
Training dataset
The dataset was datasets of medical question answering/reasoning,
summarization and medical dialogue datasets in Section 2.1.
Evaluation
Randomized OSCE was performed to understand AMIE’s capabilities
and benchmark against expert primary care physicians.
Evaluation results
Evaluation results
The results are described in Section 4.
References
1.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824–24837 (2022).
2.
Google. PaLM 2 Technical Report https://ai.google/static/documents/palm2techreport.pdf. 2023.
|46
