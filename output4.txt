 as part of professional re-validation5. We
iterated upon these criteria to refine items for inclusion and derived pilot scales and instructions for assessment
by using focus groups and interviews with clinicians and OSCE examiners based in the UK, Canada, US, and
India. Our resulting pilot framework enabled assessment from two perspectives: clinician (board-certified
3https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets
4https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf
5https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/
collecting-colleague-and-patient-feedback-for-revalidation
|9
physicians) and lay raters (patient actors). The framework included consideration of consultation quality,
structure and completeness, the roles, responsibilities, and skills of the interviewer (Tables A.1, A.2, A.3, and
A.4).
3.1 Objective Structured Clinical Examination
Objective Structured Clinical Examination (OSCE) is a practical assessment format used in healthcare
to assess clinical skills and competencies in a standardized and objective fashion [31–33]. It differs from
traditional written or oral exams that focus primarily on theoretical knowledge and instead aims to provide
an environment in which the skills of real-world clinical practice might be assessed.
The OSCE is typically divided into multiple stations (often 8-12), each simulating a real-life clinical scenario
enacted by standardized patient actors trained to portray specific symptoms or conditions based on pre-defined
scenario descriptions. At each station, students are given specific tasks to perform, such as taking a clinical
history, or making a diagnosis. Each station has a set time limit, ensuring fairness and efficient assessment.
Trained examiners observe students’ performance at each station using a pre-defined checklist or marking
scheme. They assess clinical skills like communication, history-taking, physical examination techniques, clinical
reasoning, and decision-making.
3.2 Remote OSCE Study Design
To compare AMIE’s performance to that of real clinicians, we conducted a randomized crossover study of
blinded consultations in the style of a remote OSCE. Our OSCE study involved 20 board-certified primary
care physicians (PCPs) and 20 validated patient actors, 10 each from India and Canada, respectively, to
partake in online text-based consultations. PCPs had between 3 and 25 years of post-residency experience
(median 7 years). Patient actors comprised of a mix of medical students, residents, and nurse practitioners
with experience in OSCE participation. We sourced 149 scenario packs from India (75), Canada (60), and the
UK (14).
The scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in
Canada and India), each affiliated to a medical school and with extensive experience in preparing scenario
packs and simulated patients for OSCE examinations. UK scenario packs were sourced from the samples
provided on the MRCPUK website. Each scenario pack was associated with a ground truth diagnosis and a
set of acceptable diagnoses. The scenario packs covered conditions from cardiovascular (29), respiratory (30),
gastroenterology (31), neurology (30), urology, obstetric, and gynecology domains (15), and internal medicine
(14). Pediatric or psychiatry domains were excluded from this study, as were intensive care or inpatient case
management scenarios.
Indian patient actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian
patient actors participated in scenario packs for both Canada and the other half of UK-based scenario packs.
This assignment process resulted in 149 distinct simulated patients (“scenarios”). Below, we use the term
“OSCE agent” to refer to the conversational counterpart interviewing the patient actor, i.e., either PCP or
AMIE. Table 1 summarizes the OSCE assignment information across three geographical locations. Each of
the 149 simulated patients completed the three-step study flow depicted in Figure 2.
Table 1 | OSCE study summary. Number of scenario packs, patient actors, simulated patients, and primary care physicians
(PCPs) in each of the three locations (Canada, India, and the UK) in the remote OSCE study. 20 board-certified PCPs
participated in the study as OSCE agents in comparison with AMIE, 10 each from India and Canada. 20 trained patient actors
were involved, with 10 each from India and Canada. Indian patient actors played the roles in both India and UK scenario
packs. Canadian patient actors participated in scenario packs for both Canada and the UK. This process resulted in 149 distinct
simulated patients.
Location
# of Scenario Packs
# of Simulated Patients
# of Patient Actors
# of PCPs
Canada
60
67
10
10
India
75
82
10
10
UK
14
0
0
0
Total
149
149
20
20
|10
3.2.1 Online Text-based Consultation
PCPs and patient actors were primed with sample scenarios and instructions, and participated in pilot
consultations prior to the study commencing in order to familiarize themselves with the interface and
experiment requirements.
For the experiment, each simulated patient completed two online text-based consultations via a synchronous
text chat interface (Figure A.2), one with a PCP (control) and one with AMIE (intervention). The ordering
of PCP and AMIE was randomized and patient actors were not informed as to which they were talking to in
each consultation. PCPs were located in the same country as patient actors, and were randomly drawn based
on availability at the specified time slot for the consultation. Patient actors role-played the scenario and were
instructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (PCPs
via study-specific instructions, and AMIE as part of the prompt template) to not reveal their identity, or
whether they were human, under any circumstances.
3.2.2 Post-questionnaires
Upon conclusion of the consultation, the patient actor and OSCE agent each filled in a post-questionnaire
in light of the resulting consultation transcript (Figure A.3). The post-questionnaire for patient actors
consisted of the complete GMCPQ (Table A.1), the PACES components for “Managing Patient Concerns” and
“Maintaining Patient Welfare” (Table A.2), and a checklist representation of the PCCBP category for “Fostering
the Relationship” (Table A.3). Responses patient actors provided to the post-questionnaire are referred to
as “patient actor ratings” below. The post-questionnaire for the OSCE agent asked for a ranked differential
diagnosis (DDx) list with a minimum of 3 and no more than 10 conditions, as well as recommendations for
escalation to in-person or video-based consultation, investigations, treatments, management plan, and the
need for a follow-up.
3.2.3 Specialist Physician Evaluation
Finally, a pool of 23 specialist physicians from India (14), North America (6), and the UK (3) evaluated PCPs
and AMIE with respect to the quality of their consultation, and their responses to the post-questionnaire.
During evaluation, specialist physicians also had access to the full scenario pack along with its associated
ground truth differential and additional accepted differentials. All of the data the specialist physicians had
access to during evaluation are collectively referred to as “OSCE data” below. Specialist physicians were
sourced to match the specialties and geographic regions corresponding to the scenario packs included in our
study, and had between 1 and 36 years of post-residency experience (median 5 years). Each set of OSCE data
was evaluated by one specialist physician randomly assigned to match the specialty and geographic region of
the underlying scenario (e.g., Canadian pulmonologist evaluated OSCE data from Canada-sourced respiratory
medicine scenario). Each specialist evaluated OSCE data from both PCP and AMIE for a given scenario.
Evaluations for PCP and AMIE were conducted by the same specialist in a randomized and blinded sequence.
Evaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list,
appropriateness of recommendations regarding escalation, investigation, treatment, management plan and
follow-up (Table A.4), and all PACES (Table A.2) and PCCBP (Table A.3) rating items. We also asked
specialist physicians to highlight confabulations in the consultations and questionnaire responses, i.e., text
passages that were non-factual or referred to information not provided in the conversation. Each OSCE
scenario pack additionally supplied specialists with scenario-specific clinical information to assist with rating
the clinical quality of the consultation, such as the ideal investigation or management plans; or important
aspects of the clinical history that would ideally have been elucidated for the highest quality of consultation
possible.
3.3 Auto-evaluation
In addition to human evaluations, we implemented model-based auto-evaluation methods as economical
consistent alternatives to specialist assessments. These techniques were employed to evaluate both dialogue
quality and diagnostic accuracy of the OSCE agent. To establish the validity of our auto-evaluation methods
for assessing dialogue quality, we initially focused on a subset of four evaluation axes from the PACES rubric
|11
(Table A.2) that were assessed by both the patient actors and the specialist physicians. The auto-evaluation,
which uses a self-CoT strategy (details described in Section A.9) with AMIE to rate dialogues, was in
good alignment with human raters and comparable to the inter-specialist agreement on these criteria. For
the auto-evaluation of differential diagnoses, we leveraged another LLM, Med-PaLM 2 [13] as a surrogate
for a specialist rater to grade the predicted diagnoses against the ground truth diagnoses (more details
in Section A.7). Our auto-evaluation on DDx accurac