rates other work that LLMs may be able to
produce more complete differential diagnoses given the same clinical information as physicians in challenging
cases [70]. Though not explored in this study, the assistive performance of AMIE therefore represents an
interesting and important avenue for future research, particularly given the real-world importance of expert
|17
oversight for AI systems in safety-critical settings such as medicine.
Our study utilized a wide variety of simulated patients, comprising actors trained in both Canada and India
and scenarios across a range of specialties. This allowed us to explore how performance varied along multiple
axes: by specialty, and by the locations in which the scenario was derived and enacted. We observed that both
PCPs and AMIE performed worse in obstetric/gynecology and internal medicine scenarios than those from
other specialties (see Figure A.8). The study was not powered or designed to compare performance between
different specialty topics, and we cannot exclude that the scenarios in some specialties might be harder than
others. We observed that both AMIE and PCPs had higher diagnostic accuracy in consultations performed in
the Canada OSCE lab compared to those enacted in the India OSCE lab (see Figure A.13). However, the
differences were not statistically significant and in a subset of 40 scenarios enacted in both the Canada OSCE
lab and the India OSCE lab, the performance of both AMIE and PCPs was equivalent (see Figure A.14).
Conversational Performance. Patient actors and specialist raters both evaluated AMIE’s performance
to be higher than PCPs on metrics related to empathy and communication skills. These axes comprised a
majority of the dimensions that were evaluated. This general finding is consistent with a prior study where
LLM responses were found to be more empathetic than the responses from clinicians to health questions
posted on Reddit [73]. However, the findings in that study may not be generalised directly to our setting due
to the differences in study design. Specifically, prior work has not involved a direct, randomised comparison
of physicians and AI systems in a prospective simulation of multi-turn dialogue with the same patient. In
both settings, the lack of voice-based and non-verbal visual communication may be an unfair disadvantage to
clinicians.
The text-based chat interface used in this study introduces both advantages and disadvantages. People
today most commonly engage with LLMs through synchronous text-chat interfaces [74], and patients often
use patient portals to send messages to their providers. We therefore chose this mode of interaction as a
representative interface for LLMs to perform multi-turn conversation, adapting the virtual OSCE framework
accordingly. While this allowed a fair comparison of diagnostic dialogue between LLMs and clinicians when
both were restricted to a synchronous text-chat, it is important to acknowledge that our experiments do
not emulate the expected quality of diagnostic dialogue in real clinical practice (including telemedicine).
Physicians may be more used to history-taking and diagnostic dialogue by telephone or video consultation
than synchronous text-chat communication [75, 76]. Instead, text is more commonly used by clinicians to
communicate with patients for episodic or asynchronous needs such as prescription refills or communication
about specific test results [77]. Physicians may thus be more familiar with text/SMS or email rather than the
synchronous text-chat medium we employed in this study. In both text/SMS and email, the conventions and
expectations for communicating naturally and with empathic style might be different [78]. It is possible that
the PCPs in our study had not yet become accustomed to the setting, and may have performed differently
if subjected to a specific training program (similar in spirit to the training process for AMIE). Clinicians
participating in the study undertook two preparatory pilot sessions of consultations with our synchronous
text interface before the evaluation began, but this was not a formal training program, nor was it designed
to optimize clinicians’ performance. Future research could explore this question more thoroughly including
monitoring for the impact of a learning curve, or exploring whether performance varies according to the extent
to which participating clinicians or simulated patients are familiar with telemedicine.
Additionally, our findings regarding empathic communication could also be partially attributed to the fact
that AMIE responses were significantly longer than clinician responses (shown in Figure A.11), and presented
with greater structure. This could potentially suggest to an observer that more time was spent preparing
the response, analogous to known findings that patient satisfaction increases with time spend with their
physicians [79–81].
Collectively, our findings suggest many avenues for further research that might leverage human-AI comple-
mentarity [82], combining clinicians’ skills in the analysis of verbal and non-verbal cues with the potential
strengths of LLMs to suggest more enriched conversational responses including empathic statements, structure,
eloquence, or more complete differential diagnoses.
Simulated Dialogue. The use of simulated data allowed us to quickly scale training to a broad set of
conditions and patient contexts, while the injection of knowledge from search encouraged these dialogues to
|18
remain grounded and realistic. Though the simulated patients encompassed a wide range of conditions, they
failed to capture the full range of potential patient backgrounds, personalities, and motivations. Through the
inner self-play procedure, we were able to iteratively improve the simulated dialogue we generated and used
in fine-tuning. However, these improvements were limited by our ability to articulate what makes a good
dialogue in the critic instructions, the critic’s ability to produce effective feedback, and AMIE’s ability to
adapt to such feedback. For example, in the simulated environment we impose that AMIE reaches a proposed
differential and testing/treatment plan for the patient, but such an endpoint may be unrealistic for some
conditions, especially in the virtual chat-based setting.
Evaluation Framework. In contrast to prior works, we anchored our evaluation in criteria already established
to be relevant for assessing physicians’ communication skills and history-taking quality. We performed more
extensive and diverse human evaluation than prior studies of AI systems, with ratings from both clinicians
and simulated patients perspective. Our raters and scenarios were sourced from multiple geographic locations,
including North America, India and the UK. Our pilot evaluation rubric is, to our knowledge, the first to
evaluate LLMs’ history-taking and communication skills using axes that are also measured in the real world
for physicians themselves, increasing the clinical relevance of our research. Our evaluation framework is
considerably more granular and specific than prior works on AI-generated clinical dialogue, which have not
considered patient-centred communication best practice or clinically-relevant axes of consultation quality [29,
64–68].
However, our pilot framework is not definitive and can be further improved in future research. History-taking
itself is contextual and what determines a “good history” is dependent on the specific clinical situation,
patient and physician attributes, cultural characteristics, and many other factors. Despite variation in models
for clinical history-taking [83–86], studies have shown that good clinical interviews are associated with not
only problem detection and diagnostic accuracy, but also quadruple aims for care delivery [87, 88] ranging
from patient and physician satisfaction, resilience to stress and illness, and health outcomes or cost. Future
studies on the quality of LLM history-taking might therefore utilise prospective measures of these outcomes
in real-world settings (for example reductions in patient complaints [89], or improvements in cost and care
effectiveness, patient and provider satisfaction), though evaluations as such may be challenging or impractical
to compare to standard practice in the same individual patient, and randomisation of different approaches
may also be challenging in real-world settings.
Breadth of Evaluation. Our chosen axes of evaluation were not exhaustive and their interpretation was
often subjective in nature. Although we conducted evaluations from both clinician and lay-perspectives,
generating scenario-packs in three countries with assessors in both North America and India, the pool of
clinicians and lay-people assessing the models could be expanded further to improve generalization of our
insights. Our experiments could also undergo more extensive replication to explore other aspects such as
inter-observer and inter-participant variability, including future work with an intentionally further diversified
pool of human raters (clinicians and lay users). Participatory design in the development of model evaluation
tools with a representative pool of patients, as well as clinical and health equity domain experts, could also be
valuable.
Although our scenarios comprised many different clinical conditions and specialties, our experiments were not
necessarily representative of the decades of clinical practice accumulated by even a single doctor (who on
average may perform tens of thousands of consultations in a career [90]). The range of conditions possible
to examine in medicine is vast as is the variation in presentation of individual diseases. Our experiments
were not designed to examine multi-morbidity and co-incident pathology, longitudinal case presentation or
the consideration of sequential information from clinical investigations. We excluded entirely some clinical
settings or specialties such as psychiatry, pediatrics