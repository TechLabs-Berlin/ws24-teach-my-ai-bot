y showed a similar trend for AMIE and OSCE agents
compared to the specialist ratings. Overall, auto-evaluation trends aligned with human ratings for both
dialogue quality and diagnostic accuracy.
We also conducted additional auto-evaluation analyses for the following purposes:
• To compare the performance of the DDx accuracy derived from AMIE or PCP consultations;
• To compare the DDx accuracy between simulated patients performed in Canada and India and determine
if there is systematic differences between the two locations;
• To isolate the effects of information acquisition and information interpretation by analyzing the DDx
accuracy of AMIE when provided the PCP consultation instead of its own;
• To evaluate the efficiency of information acquisition between AMIE and PCPs by analyzing the DDx
accuracy as the number of conversation turns increases;
• To evaluate the benefit of inner-loop self-play on dialogue quality before and after critic feedback.
3.4 Statistical Analysis
We evaluated the top-k accuracy of the DDx lists generated by AMIE and PCPs across all 149 simulated
patients. Top-k accuracy was defined as the percentage of cases where the correct diagnosis appeared within
the top-k positions of the DDx list. Specifically, a candidate diagnosis was considered a match if the specialist
rater marked it as either an exact match with, very close to or closely related to the ground truth diagnosis
(or accepted differential). Statistical significance for DDx accuracy was determined using bootstrap tests [34]
with 10,000 samples and false discovery rate (FDR) correction [35] across all k. Statistical significance for
patient actor and specialist ratings was determined using Wilcoxon signed-rank tests [36] FDR correction.
Cases where either agent received “Cannot rate / Does not apply” were excluded from the test. Results below
refer to p-values after FDR correction.
4 Results
4.1 Diagnostic Accuracy
4.1.1 AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.
AMIE’s diagnostic accuracy was assessed as higher than that of PCPs. Figure 3 shows the top-k accuracy for
AMIE and PCPs, considering matches with the ground truth diagnosis (a) and matches with any item on
the accepted differential (b). AMIE showed significantly higher top-k accuracy than that of PCPs across all
values of k (p < 0.05). Note that unlike AMIE, PCPs did not always provide 10 diagnoses in their differential
diagnoses (min: 3, mean: 5.39). Additionally, we performed a comparison of DDx accuracy between AMIE
and PCP by varying the matching criteria for determining a match. Results depicted in Figure A.7 further
substantiate AMIE’s superior DDx performance across various matching criteria.
Accuracy by Specialty. Figure A.8 illustrates the DDx accuracy achieved by AMIE and PCPs across the
six medical specialties covered by scenarios in our study. We observed that AMIE’s performance matched or
surpassed PCP performance for all specialties with the most pronounced improvements in the respiratory and
cardiovascular specialities.
4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information.
Auto-evaluation Accuracy. We reproduced the DDx accuracy analysis with our model-based auto-evaluator
instead of the specialist raters using the same procedure as in Figure 3. The overall performance trends
obtained through the auto-evaluator align well with specialist assessments despite marginal differences in the
|12
a
b
Figure 3 | Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k DDx accuracy are compared across 149
scenarios with respect to the ground truth diagnosis (a) and all diagnoses in the accepted differential (b). Bootstrapping
(n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p < 0.05 after FDR
correction.
computed accuracy values, as shown in Figure A.9.
Isolating the Source of Performance Gains. To investigate whether AMIE’s superior DDx performance
observed in Figure 3 stemmed from improved information acquisition or from better diagnostic reasoning
capability, we compared AMIE’s diagnoses based on its own consultations with AMIE’s diagnoses generated
from the corresponding PCP consultations, using the DDx auto-evaluator. Results depicted in Figure A.10
revealed markedly similar DDx performance, indicating that the diagnostic performance remained consistent
regardless of whether AMIE processed information from its own dialogue or from the PCP’s conversation. Both
methods significantly outperformed the differential diagnoses produced by PCPs. These results suggest that
AMIE was approximately equivalent to PCPs at information acquisition but better than PCPs at interpreting
that information to produce an accurate/complete differential diagnosis.
Efficiency of Information Acquisition. Although AMIE displayed greater verbosity compared to PCPs
in terms of total number of words generated in their responses during the consultation, the number of
conversational turns and the number of words elicited from the patient actors were similar across both OSCE
agents, as illustrated in Figure A.11. This suggests that both AMIE and PCPs acquired a similar amount
of information from the patients during the encounter. To investigate how efficient AMIE or PCPs were at
gathering sufficient information to formulate a correct diagnosis, we truncated the conversations at various turn
counts and used AMIE to generate differential diagnoses based on these partial conversations. Figure A.12
depicts the top-3 DDx accuracy as a function of the number of turns provided to the model. The observed
accuracies plateaued within the initial 10 conversational turns for both AMIE and PCPs. This suggests that
both AMIE and PCPs were able to acquire the information necessary for formulating a diagnosis within
the early stages of the conversation. Additionally, the comparable performance at every turn indicates that
neither AMIE nor PCPs had a significant advantage in the efficiency or quality of information acquisition.
4.2 Conversation Quality
4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors.
Conversation quality was assessed using patient actor ratings, specialist ratings, and outputs from auto-
evaluation. Figure A.5 and A.6 show two example consultations for the same simulated patient from AMIE
and PCP, respectively.
|13
0
20
40
60
80
100
% Consultations
Patient Happy to
Return In Future (Y/N)
Patient Confident About
Care Provided (Y/N)
Appearing Honest
And Trustworthy
Patient Trusts Information
is Confidential
Providing Appropriate
Treatment Plan
Involving Patient in
Treatment Decisions
Explaining Condition
and Treatment
Assessesing
Medical Condition
Listening to Patient
Making Patient
Feel At Ease
Being Polite
***
N=132
***
N=142
***
N=127
***
N=118
***
N=120
***
N=131
***
N=143
***
N=146
***
N=145
***
N=146
***
N=147
GMCPQ
0
20
40
60
80
100
% Consultations
Valueing Patient
as A Person (Y/N)
Encouraging Patient
Participation (Y/N)
Using Appropriate
Language (Y/N)
Greeting Patient
Appropriately (Y/N)
Acknowledging
Mistakes (Y/N)
Expressing Caring
And Commitment (Y/N)
Engaging In Partnership
Building (Y/N)
Respecting Patient's
Privacy (Y/N)
Discussing Roles And
Responsibilities (Y/N)
Appearing Open
And Honest (Y/N)
Building Rapport
And Connection (Y/N)
***
N=122
***
N=128
***
N=145
**
N=140
n.s.
N=41
***
N=136
***
N=115
n.s.
N=108
***
N=118
*
N=115
***
N=133
PCCBP
0
20
40
60
80
100
% Consultations
Maintaining
Patient Welfare
Showing Empathy
Understanding
Patient Concerns
Addressing
Patient Concerns
***
N=145
***
N=146
***
N=146
***
N=147
Managing
Patient Concerns
PACES
AMIE (top)
PCP (bottom)
 
Very favorable
Favorable (or "Yes" for Y/N)
Neither favorable nor unfavorable
Unfavorable (or "No" for Y/N)
Very unfavorable
Cannot rate / Does not apply
Figure 4 | Patient actor ratings. Conversation qualities as assessed by patient actors upon conclusion of the consultation.
For illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from ‘Very
favorable’ to ‘Very unfavorable’. For Yes/No questions, a (positive) ‘Yes’ response was mapped to the same color as ‘Favorable’
and a (negative) ’No’ response to the same color as ‘Unfavorable’. Rating scales were adapted from the General Medical Council
Patient Questionnaire (GMCPQ), the Practical Assessment of Clinical Examination Skills (PACES), and a narrative review
about Patient-Centered Communication Best Practice (PCCBP). Details on question wording and response options are provided
in Section A.1. Asterisks represent statistical significance (∗: p < 0.05, ∗∗: p < 0.01, ∗∗∗: p < 0.001, n.s. : not significant).
|14
Patient Actor Ratings. Figure 4 presents the various conversation qualities patient actors assessed following
their consultations with the OSCE agents. Overall, AMIE’s consultations were rated significantly better
(p < 0.05) by patient actors than those from PCPs across 24 of 26 axes. No significant differences in ratings
were detected for the two PCCBP axes “Respecting Patient’s Privacy” (N=108) and “Acknowledging Mistakes”
(N=41). For the latter criterion, the number of exclusions was substantially higher since the question applied
only when mistakes were made by the OSCE agent and pointed out in the conversation.
Specialist Physician Ratings. Specialist physicians evaluated both the conversational quality as well as
the responses to the post-questionnaire for scenarios within their domain expertise (see Figure 5). Again,
AMIE’s responses were rated significantly better by specialists than those from PCPs on 28 of 32 evaluation
axes; Specialists preferred AMIE’s consultation, diagnoses, and management plan over those from PCPs. For
this set of evaluations, differences in specialist ratings between AMIE and PCPs were statistically significant
(p < 0.05).
No significant differences in ratings were detected for four of the 