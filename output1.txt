Towards Conversational Diagnostic AI
Tao Tu∗,1, Anil Palepu∗,1, Mike Schaekermann∗,1,
Khaled Saab1, Jan Freyberg1, Ryutaro Tanno2, Amy Wang1, Brenna Li1, Mohamed Amin1,
Nenad Tomasev2, Shekoofeh Azizi2, Karan Singhal1, Yong Cheng2, Le Hou1, Albert Webson2,
Kavita Kulkarni1, S. Sara Mahdavi2, Christopher Semturs1,
Juraj Gottweis1, Joelle Barral2, Katherine Chou1, Greg S. Corrado1, Yossi Matias1,
Alan Karthikesalingam†,1 and Vivek Natarajan†,1
1Google Research, 2Google DeepMind
At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for
accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable
of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating
clinicians’ expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical
Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.
AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling
learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating
clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management
reasoning, communication skills, and empathy. We compared AMIE’s performance to that of primary
care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with
validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study
included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison
with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater
diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of
26 axes according to patient actors. Our research has several limitations and should be interpreted with
appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale
LLM-patient interactions but is not representative of usual clinical practice. While further research is
required before AMIE could be translated to real-world settings, the results represent a milestone towards
conversational diagnostic AI.
1 Introduction
The dialogue between the physician and the patient is fundamental to effective and compassionate care. The
medical interview has been termed “the most powerful, sensitive, and most versatile instrument available
to the physician” [1]. In some settings, it is believed that 60-80% of diagnoses are made through clinical
history-taking alone [2–6]. The physician-patient dialogue extends beyond history-taking and diagnosis; it
is a complex interaction which establishes rapport and trust, serves as a tool for addressing health needs
and can empower patients to make informed decisions that account for their preferences, expectations, and
concerns [7]. Clinicians wield considerable skills in clinical history-taking and the wider “diagnostic dialogue”,
but access to this expertise remains episodic and globally scarce [8].
Recent progress in general-purpose large language models (LLMs) [9–11] has shown that artificial intelli-
gence (AI) systems have capabilities to plan, reason, and incorporate relevant context to hold naturalistic
conversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards
the development of fully interactive conversational AI. Such medical AI systems would understand clinical
language, intelligently acquire information under uncertainty, and engage in natural, diagnostically useful
medical conversations with patients and those who care for them. The potential real-world utility of AI
systems capable of clinical and diagnostic dialogue is broad, as the development of such capabilities might
improve access to diagnostic and prognostic expertise, to improved quality, consistency, availability, and
affordability of care, and to help realize better health outcomes (particularly for populations facing healthcare
disparities).
∗Equal contributions. † Equal leadership.
‡ Corresponding authors: {taotu, mikeshake, alankarthi, natviv}@google.com
arXiv:2401.05654v1  [cs.AI]  11 Jan 2024
Inference 
Reasoning Chain
Data
Medical Reasoning
Real-world Dialogue
Medical 
Summarization
Long-form 
Medical QA
Simulated Dialogue
Analyze 
Context
Generate 
Response
Refine 
Response
Critic
AMIE
Fine-tuning
Simulated Dialogue Generator
Outer Self-play
Moderator
Simulated 
Dialogue
Patient Agent
Inner Self-play
Doctor Agent
Critic
Vignette 
Generator
AI
Patient
Actor
Scenario
Pack
Simulated Patient
Primary Care
Physician 
(PCP) 
AMIE
Randomized
Top-3 Diagnostic 
Accuracy
Patient’s Confidence 
in Care
Management 
Plan
Escalation 
Recommendation
Empathy
Perceived Openness 
& Honesty
AMIE System Design
Randomized Study Design for Remote
Objective Structured Clinical Examination (OSCE)
AMIE Outperforms PCPs on 
Multiple Evaluation Axes for Diagnostic Dialogue
AI
PCP
AMIE
Text Conversation
Specialist Physician 
Perspective
Patient Actor 
Perspective
Auto-evaluation
Figure 1 | Overview of contributions. AMIE is a conversational medical AI optimised for diagnostic dialogue. AMIE is
instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical
reasoning, question answering, and summarization datasets. Notably, we designed a self-play based simulated dialogue environment
with automated feedback mechanisms to scale AMIE’s capabilities across various medical contexts and specialities. Specifically,
this iterative self-improvement process consisted of two self-play loops: (1) An “inner” self-play loop, where AMIE leveraged
in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; (2) An “outer” self-play loop
where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. During online inference,
AMIE used a chain-of-reasoning strategy to progressively refine its response conditioned on the current conversation to arrive at
an accurate and grounded reply to the patient in each dialogue turn. We designed and conducted a blinded remote Objective
Structured Clinical Examination (OSCE) with validated simulated patient actors interacting with AMIE or Primary Care
Physicians (PCPs) via a text interface. Across multiple axes corresponding to both specialist physician (28 out of 32) and patient
actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest.
However, while LLMs have been shown to encode clinical knowledge and proven capable of highly accurate
single-turn medical question-answering [12–14], their conversational capabilities have been tailored to domains
outside clinical medicine [15, 16]. Prior work in LLMs for health [12–14, 17, 18] has not yet rigorously
examined the clinical history-taking and diagnostic dialogue capabilities of AI systems or contextualized this
by comparison to the extensive capabilities of expert clinicians.
Clinical history-taking and diagnostic dialogue through which clinicians derive diagnosis and management plans
represent a complex skill [19] whose optimal conduct is highly dependent on context. Thus, multiple evaluation
axes are needed to assess the quality of a diagnostic dialogue, including the structure and completeness of
|2
the elicited history, diagnostic accuracy, the appropriateness of management plans and their rationale, and
patient-centred considerations such as relationship-building, respect for the individual and communication
efficacy [20]. If the conversational potential of LLMs is to be realized in medicine, there is a significant unmet
need to better optimize development and evaluation of medical AI systems for characteristics such as these,
which are unique to history-taking and diagnostic dialogue between clinicians and patients.
In this work, we detail our progress towards a conversational medical AI system for clinical history-taking and
diagnostic reasoning.
Our key contributions are summarized as:
• We introduced AMIE (Articulate Medical Intelligence Explorer), an LLM based AI system optimized for
clinical history-taking and diagnostic dialogue.
• To scale AMIE across a multitude of specialties and scenarios, we developed a novel self-play based
simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate
its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE’s
diagnostic accuracy and conversation quality.
• We developed a pilot evaluation rubric to assess the history-taking, diagnostic reasoning, communication
skills and empathy of diagnostic conversational medical AI, encompassing both clinician-centred and
patient-centred metrics.
• We designed and conducted a blinded remote OSCE study with 149 case scenarios from clinical providers
in Canada, the UK, and India, enabling randomized and counterbalanced comparison of AMIE to PCPs
when performing consultations with validated patient actors. AMIE exhibited superior diagnostic accuracy
compared to PCPs as assessed by various measures (e.g., top-1 and top-3 accuracy of the differential
diagnosis list). Across 28 out of 32 evaluation axes from the specialist physician perspective and 24 out
of 26 evaluation axes from the patient actor perspective, AMIE was rated superior to PCPs while being
non-inferior on the rest.
• We performed a range of ablations to further understand and characterize the capabilities of AMIE,
highlighted important limitations, and proposed key next steps for real-world clinical translation of AMIE.
Our research has important limitations, most notably that we utilized a text-chat interface, which although
enabling potentially large-sc